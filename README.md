# Transformer Chatbot Training  

This repository contains the training pipeline for a **Transformer-based Chatbot** using PyTorch and the Hugging Face `transformers` library. The chatbot is based on **GPT-2** and trained on a dataset of synthetic conversations.  

---

## ğŸš€ **Project Structure**  

```
oCray/
â”‚â”€â”€ data/  
â”‚   â”œâ”€â”€ dataset_loader.py  # Loads and preprocesses the dataset  
â”‚   â”œâ”€â”€ raw/dataset.jsonl  # The dataset (JSONL format)  
â”‚  
â”‚â”€â”€ models/  
â”‚   â”œâ”€â”€ transformer.py  # TransformerChatbot model  
â”‚   â”œâ”€â”€ chatbot_model.pth  # Trained model weights (after training)  
â”‚  
â”‚â”€â”€ training/  
â”‚   â”œâ”€â”€ trainer.py  # Training script  
â”‚  
â”‚â”€â”€ README.md  # This file  
```

---

## ğŸ“‚ **Dataset Format**  
The dataset is stored in **JSONL (JSON Lines) format** at `data/raw/dataset.jsonl`.  
Each line is a separate JSON object with the following structure:  

```json
{"input persona": "Friendly AI", "synthesized text": "Hello! How can I assist you today?"}
```

- **`input persona`**: The persona or character of the chatbot.  
- **`synthesized text`**: The chatbot's response based on that persona.  

---

## ğŸ›  **How It Works**  

### 1ï¸âƒ£ **Dataset Loader (`data/dataset_loader.py`)**  
- Loads the dataset from `dataset.jsonl`  
- Uses a tokenizer to convert text into tokenized tensors  
- Returns a PyTorch `DataLoader` for batching  

```python
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # Set padding token for GPT-2

dataloader = get_dataloader("data/raw/dataset.jsonl", tokenizer, batch_size=4)
```

---

### 2ï¸âƒ£ **Model Definition (`models/transformer.py`)**  
- The chatbot is implemented in `TransformerChatbot` (based on GPT-2).  
- If you're using a custom model, ensure it returns logits of shape `(batch_size, seq_len, vocab_size)`.  

---

### 3ï¸âƒ£ **Training Script (`training/trainer.py`)**  
The script:  
âœ… Loads the dataset  
âœ… Initializes the GPT-2 model  
âœ… Trains for **3 epochs** using `Adam` optimizer  
âœ… Saves the trained model at `models/chatbot_model.pth`  

#### ğŸ”¹ **Training Loop**  
```python
for epoch in range(epochs):
    total_loss = 0
    model.train()

    for batch in dataloader:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["input_ids"].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss  # Model's built-in loss

        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader)}")
```

---

## ğŸƒâ€â™‚ï¸ **How to Train the Model**  

### 1ï¸âƒ£ **Install Dependencies**  
Make sure you have **Python 3.8+** and install required libraries:  
```bash
pip install torch transformers pandas
```

### 2ï¸âƒ£ **Run the Training Script**  
```bash
python training/trainer.py
```

### 3ï¸âƒ£ **Save and Load Model**  
After training, the model is saved at:  
```
models/chatbot_model.pth
```
To load the model for inference:  
```python
model.load_state_dict(torch.load("models/chatbot_model.pth"))
model.eval()  # Set to evaluation mode
```

---

## ğŸ¤– **Next Steps**  
- Improve dataset quality for better chatbot responses  
- Fine-tune on larger conversation datasets  
- Deploy the chatbot using FastAPI or Flask  

---

## ğŸ **License**  
This project is open-source under the **MIT License**.  
Feel free to modify and improve! ğŸš€

